{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import glob\n",
    "\n",
    "# 配置信息\n",
    "PDF_FILES_PATH = 'KB3/pdfs/*.pdf'\n",
    "TXT_FILES_PATH = 'KB3/pdfs/test1.txt'\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "FAISS_DB_PATH = 'faiss_db'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "\n",
    "def load_and_split_files(pdf_path, txt_path):\n",
    "    try:\n",
    "        pdf_files = glob.glob(pdf_path)\n",
    "        txt_files = glob.glob(txt_path)\n",
    "\n",
    "        if not pdf_files and not txt_files:\n",
    "            print(f\"未找到符合路径 {pdf_path} 的 PDF 文件和符合路径 {txt_path} 的 TXT 文件。\")\n",
    "            return []\n",
    "\n",
    "        all_docs = []\n",
    "\n",
    "        # # 加载 PDF 文件\n",
    "        # for file in pdf_files:\n",
    "        #     loader = PDFPlumberLoader(file)\n",
    "        #     docs = loader.load()\n",
    "        #     all_docs.extend(docs)\n",
    "\n",
    "        # 加载 TXT 文件\n",
    "        for file in txt_files:\n",
    "            loader = TextLoader(file)\n",
    "            docs = loader.load()\n",
    "            all_docs.extend(docs)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=800,\n",
    "                                                       add_start_index=True)\n",
    "        all_splits = text_splitter.split_documents(all_docs)\n",
    "        print(f\"成功加载并分割 {len(all_splits)} 个文本块。\")\n",
    "        return all_splits\n",
    "    except Exception as e:\n",
    "        print(f\"加载和切分文件时出错: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def build_vector_store():\n",
    "    all_splits = load_and_split_files(PDF_FILES_PATH, TXT_FILES_PATH)\n",
    "    if not all_splits:\n",
    "        return None\n",
    "    try:\n",
    "        local_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)\n",
    "        vectorstore = FAISS.from_documents(documents=all_splits, embedding=local_embeddings,normalize_L2=True)\n",
    "\n",
    "        vectorstore.save_local(FAISS_DB_PATH)\n",
    "        print(\"向量存储构建完成并持久化到目录\")\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"初始化向量存储时出错: {e}\")\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载并分割 2 个文本块。\n",
      "向量存储构建完成并持久化到目录\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7366e4d64fe0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_vector_store()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
